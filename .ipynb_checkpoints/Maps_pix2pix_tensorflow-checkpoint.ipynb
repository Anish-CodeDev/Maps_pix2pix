{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maps_pix2pix_tensorflow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmhRC_2qkIfr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"maps\"\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    fname=f'{dataset_name}.tar.gz',\n",
        "    origin=f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz',\n",
        "    extract=True\n",
        ")\n",
        "path_to_zip = Path(path_to_zip)\n",
        "path = path_to_zip.parent/dataset_name"
      ],
      "metadata": {
        "id": "dRIj5vQ-kZtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(image_file):\n",
        "  image_file = tf.io.read_file(image_file)\n",
        "  image_file = tf.image.decode_jpeg(image_file)\n",
        "  w = tf.shape(image_file)[1]\n",
        "  w = w//2\n",
        "  input_image = image_file[:,:w,:]\n",
        "  output_image = image_file[:,w:,:]\n",
        "  input_image = tf.cast(input_image,dtype=tf.float32)\n",
        "  output_image = tf.cast(output_image,dtype=tf.float32)\n",
        "  return input_image,output_image"
      ],
      "metadata": {
        "id": "jYFO2lO5lXhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE=1\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH=256"
      ],
      "metadata": {
        "id": "rDzXxU8Ml21L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(input_img,target_img,height,width):\n",
        "  input_img = tf.image.resize(input_img,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  target_img = tf.image.resize(target_img,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  return input_img,target_img"
      ],
      "metadata": {
        "id": "G0nzbpVOmcTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_crop(input_img,target_img):\n",
        "  stacked_image = tf.stack([input_img,target_img],axis=0)\n",
        "  cropped_image = tf.image.random_crop(stacked_image,size=[2,IMG_HEIGHT,IMG_WIDTH,3])\n",
        "  return cropped_image[0],cropped_image[1]"
      ],
      "metadata": {
        "id": "NoUXhlG_nKGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(input_img,target_img):\n",
        "  input_img = (input_img/127.5)-1\n",
        "  target_img = (target_img/127.5)-1\n",
        "  return input_img,target_img"
      ],
      "metadata": {
        "id": "LmzcgEJIni8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_jitter(input_img,real_img):\n",
        "  input_img,real_img = resize(input_img,real_img,286,286)\n",
        "  input_img,real_img = random_crop(input_img,real_img)\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    input_img = tf.image.flip_left_right(input_img)\n",
        "    real_img = tf.image.flip_left_right(real_img)\n",
        "  return input_img,real_img  "
      ],
      "metadata": {
        "id": "pyF3VWG0n1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_train_dataset(ds):\n",
        "  input_img,real_img = load(ds)\n",
        "  input_img,real_img = random_jitter(input_img,real_img)\n",
        "  input_img,real_img = normalize(input_img,real_img)\n",
        "  return input_img,real_img"
      ],
      "metadata": {
        "id": "Sfah-RFLo2J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_dataset(ds):\n",
        "  input_img,real_img = load(ds)\n",
        "  input_img,real_img = resize(input_img,real_img,IMG_HEIGHT,IMG_WIDTH)\n",
        "  input_img,real_img = normalize(input_img,real_img)\n",
        "  return input_img,real_img"
      ],
      "metadata": {
        "id": "N4LnN00Apj3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.list_files(str(path/\"train/*.jpg\"))\n",
        "train_ds = train_ds.map(load_train_dataset,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "GKcKfgeuqReO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.data.Dataset.list_files(str(path/\"val/*.jpg\"))\n",
        "test_ds = test_ds.map(load_test_dataset,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.shuffle(BUFFER_SIZE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "OMHvYyBtwX-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters,size,apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0.,0.02)\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(filters,size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False))\n",
        "  if apply_batchnorm:\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  return model  "
      ],
      "metadata": {
        "id": "826m3QjowpzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(filters,size,apply_dropout=False):\n",
        "  initializer=tf.random_normal_initializer(0.,0.02)\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(filters,size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  if apply_dropout:\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.ReLU())\n",
        "  return model  "
      ],
      "metadata": {
        "id": "77hjrJ0bzM3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.Input(shape=[256,256,3])\n",
        "  down_stacks = [\n",
        "    downsample(64,4,apply_batchnorm=False),\n",
        "    downsample(128,4),\n",
        "    downsample(256,4),\n",
        "    downsample(512,4),\n",
        "    downsample(512,4),\n",
        "    downsample(512,4),\n",
        "    downsample(512,4),\n",
        "    downsample(512,4)\n",
        "  ]\n",
        "  up_stacks = [\n",
        "    upsample(512,4,apply_dropout=True),\n",
        "    upsample(512,4,apply_dropout=True),\n",
        "    upsample(512,4,apply_dropout=True),\n",
        "    upsample(512,4),\n",
        "    upsample(256,4),\n",
        "    upsample(128,4),\n",
        "    upsample(64,4)\n",
        "  ]\n",
        "  x  = inputs\n",
        "  skips = []\n",
        "  for down in down_stacks:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "  skips = reversed(skips[:-1])\n",
        "  for up,skip in zip(up_stacks,skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x,skip])\n",
        "  initializer = tf.random_normal_initializer(0.,0.02)    \n",
        "  last = tf.keras.layers.Conv2DTranspose(3,4,strides=2,kernel_initializer=initializer,use_bias=False,padding='same',activation='tanh')\n",
        "  x = last(x)\n",
        "  return tf.keras.Model(inputs=inputs,outputs=x)"
      ],
      "metadata": {
        "id": "Kx2pFO91z6pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()"
      ],
      "metadata": {
        "id": "mM2pz5U_jqcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAMBDA = 100\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "qwR0NrNb2IsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_loss_func(disc_generated_output,gen_output,target):\n",
        "  gen_loss  = loss_obj(tf.ones_like(disc_generated_output),disc_generated_output)\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target-gen_output))\n",
        "  total_loss = gen_loss + (LAMBDA*l1_loss)\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "pFpbX7PJ2EJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer()\n",
        "  inp = tf.keras.layers.Input(shape=[256,256,3],name='inp')\n",
        "  tar = tf.keras.layers.Input(shape=[256,256,3],name='tar')\n",
        "  x = tf.keras.layers.concatenate([inp,tar])\n",
        "  down1 = downsample(64,4,apply_batchnorm=False)(x)\n",
        "  down2 = downsample(128,4)(down1)\n",
        "  down3 = downsample(256,4)(down2)\n",
        "  zero_padding_2D_1 = tf.keras.layers.ZeroPadding2D()(down3)\n",
        "  conv = tf.keras.layers.Conv2D(512,4,strides=1,kernel_initializer=initializer,use_bias=False,padding='same',activation='tanh')(zero_padding_2D_1)\n",
        "  batch_norm_1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "  relu = tf.keras.layers.LeakyReLU()(batch_norm_1)\n",
        "  zero_padding_2D_2 = tf.keras.layers.ZeroPadding2D()(relu)\n",
        "  last = tf.keras.layers.Conv2D(1,4,strides=1,kernel_initializer=initializer,padding='same',use_bias=False)(zero_padding_2D_2)\n",
        "  return tf.keras.Model(inputs=[inp,tar],outputs=last)"
      ],
      "metadata": {
        "id": "TP8-Y8TB3dYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator()"
      ],
      "metadata": {
        "id": "tvJPexREjy0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def disc_loss_func(real_loss,fake_loss):\n",
        "  real_loss = loss_obj(tf.ones_like(real_loss),real_loss)\n",
        "  fake_loss = loss_obj(tf.zeros_like(fake_loss),fake_loss)\n",
        "  total_loss = real_loss+fake_loss\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "4GK_4nAz6V2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3,beta_1=0.5)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3,beta_1=0.5)"
      ],
      "metadata": {
        "id": "8CSZre9e6psm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model,test_input,tar):\n",
        "  pred = model(test_input,training=True)\n",
        "  display_list = [test_input[0],pred[0],tar[0]]\n",
        "  title = ['Input Image', 'Predicted Image', 'Ground Truth']\n",
        "  plt.figure(figsize=(15,15))\n",
        "  for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(display_list[i]*0.5+0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()  \n",
        "  "
      ],
      "metadata": {
        "id": "mt171btn7FnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(image,target):\n",
        "  with tf.GradientTape() as gen_tape,tf.GradientTape() as disc_tape:\n",
        "    gen_out = generator(image,training=True)\n",
        "    disc_gen = discriminator([image,gen_out],training=True)\n",
        "    disc_true = discriminator([image,target],training=True)\n",
        "    gen_loss = gen_loss_func(disc_gen,gen_out,target)\n",
        "    disc_loss = disc_loss_func(disc_true,disc_gen)\n",
        "    disc_gradient = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
        "    gen_gradient = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
        "    disc_optimizer.apply_gradients(zip(disc_gradient,discriminator.trainable_variables))\n",
        "    gen_optimizer.apply_gradients(zip(gen_gradient,generator.trainable_variables))  "
      ],
      "metadata": {
        "id": "iLQoH0As9zGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_ds,test_ds,steps):\n",
        "  example_inp,example_target = next(iter(test_ds.take(1)))\n",
        "  for step,(input_image,target) in train_ds.repeat().take(steps).enumerate():\n",
        "    start = time.time()\n",
        "    if step % 1000 ==0:\n",
        "      if step!=0:\n",
        "        print(f'Time taken for 1000 steps: {time.time()-start}')\n",
        "      start = time.time()\n",
        "      generate_images(generator,example_inp,example_target)\n",
        "      print(f'Step: {step//1000}')\n",
        "    train_step(input_image,target)\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.',end='',flush=True)      \n"
      ],
      "metadata": {
        "id": "0Q5phy-vf59i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(train_ds,test_ds,40000)"
      ],
      "metadata": {
        "id": "MGkVs2OalET6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=gen_optimizer,\n",
        "    discriminator_optimizer=disc_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator\n",
        ")"
      ],
      "metadata": {
        "id": "utkZMMFK4p2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "70OhCFgs5TOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "id": "47XZkYSx5tqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cjyZaLLF6XmU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}